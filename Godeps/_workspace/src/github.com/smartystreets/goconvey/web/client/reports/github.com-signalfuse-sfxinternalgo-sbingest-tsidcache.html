
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/signalfuse/sfxinternalgo/sbingest/tsidcache/tsidcache.go (100.0%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" >package tsidcache

import (
        "bytes"

        "sync/atomic"

        "time"

        "errors"

        "sync"

        "git.apache.org/thrift.git/lib/go/thrift"
        "github.com/Sirupsen/logrus"
        "github.com/signalfuse/sfxinternalgo/sbingest/datum"
        "github.com/signalfuse/sfxinternalgo/sbingest/datum/datumsink"
        "github.com/signalfuse/sfxinternalgo/sbingest/tsidcache/tsiddiskcache"
        "github.com/signalfuse/sfxinternalgo/sfxthrift"
        "github.com/signalfuse/sfxinternalgo/sfxthrift/thrift/datamodel"
        "github.com/signalfuse/sfxinternalgo/sfxthrift/thrift/id"
        "github.com/signalfuse/sfxinternalgo/sfxthrift/thrift/metabase"
        "github.com/signalfx/golib/datapoint"
        "github.com/signalfx/golib/distconf"
        "github.com/signalfx/golib/eventcounter"
        "github.com/signalfx/golib/logherd"
        "github.com/signalfx/golib/sfxclient"
        "github.com/signalfx/golib/timekeeper"
        "golang.org/x/net/context"
)

var log *logrus.Logger

func init() <span class="cov8" title="1">{
        log = logherd.New()
}</span>

// TimeSeriesCreator is the minimal interface the tsidcache needs to suppor to create timeseries.
// This is usually just datamodel thrift
type TimeSeriesCreator interface {
        CreateManyTimeSeries(metricTimeSeries map[*datamodel.MetricTimeSeriesDetails]bool, eventTimeSeries map[*datamodel.EventTimeSeriesDetails]bool, awaitPropagation bool) (r *datamodel.CreateManyResult_, err error)
        CheckHealth() (err error)
}

type orgTimeoutTracker struct {
        orgsInTimeout   map[int64]time.Time
        timeoutDuration *distconf.Duration
        mu              sync.RWMutex
}

// IsOrgInTimeout returns true if the given org should be in timeout
func (o *orgTimeoutTracker) IsOrgInTimeout(now time.Time, orgID int64) bool <span class="cov8" title="1">{
        o.mu.RLock()
        timeoutUntil, exists := o.orgsInTimeout[orgID]
        o.mu.RUnlock()
        if !exists </span><span class="cov8" title="1">{
                return false
        }</span>
        <span class="cov8" title="1">if timeoutUntil.After(now) </span><span class="cov8" title="1">{
                return true
        }</span>
        <span class="cov8" title="1">o.mu.Lock()
        defer o.mu.Unlock()
        delete(o.orgsInTimeout, orgID)
        return false</span>
}

// PutOrgInTimeout puts an org in timeout
func (o *orgTimeoutTracker) PutOrgInTimeout(now time.Time, orgID int64) <span class="cov8" title="1">{
        timeoutDuration := o.timeoutDuration.Get()
        if timeoutDuration.Nanoseconds() == 0 </span><span class="cov8" title="1">{
                return
        }</span>
        <span class="cov8" title="1">o.mu.Lock()
        defer o.mu.Unlock()
        o.orgsInTimeout[orgID] = now.Add(timeoutDuration)</span>
}

type cacheStats struct {
        PointDroppedContextClosed    int64
        DiskCacheHits                int64
        DiskCacheMiss                int64
        DiskCacheErrRead             int64
        DiskCacheErrWrite            int64
        TotalMtsCreationCalls        int64
        TotalMtsCreated              int64
        TotalMtsCreationErrors       int64
        TotalMtsDroppedByBadCreation int64
        TotalMtsCreationExceptions   int64

        TotalItemResults        [5]int64
        TotalItemResultsUnknown int64

        PointsDroppedPerStatus     [5]int64
        PointsDroppedUnknownStatus int64

        TotalMtsDroppedByExceptionCreation int64
        DropNoTsAfterCreate                int64
        DropBacklogFull                    int64
        TotalPointsWaitingToBeCreated      int64

        TotalDroppedInTimeout               int64
        NumThreadUpdates                    int64
        NumCurrentlyRunningMtsCreationLoops int64
}

// DiskStorage is implemented by any backing that can store tsid onto disk to let us later search
// for them
type DiskStorage interface {
        Tsid(buf *bytes.Buffer, k []tsiddiskcache.Encodeable) ([]int64, error)
        WriteResult(buf *bytes.Buffer, res map[tsiddiskcache.Encodeable]int64) error
        MarkValidAtTimestamp(now time.Time) error
        ValidAtTimestamp() (t time.Time, err error)
}

// InjectTsidOnDatum forwards datum with their TSID injected.
type InjectTsidOnDatum struct {
        diskCache               DiskStorage
        datumOutput             datumsink.Sink
        closeChan               chan struct{}
        spawnedGoroutinesClosed sync.WaitGroup
        maxWaitSize             *distconf.Int
        logLimiter              eventcounter.EventCounter
        timeoutTracker          orgTimeoutTracker

        // Note: Keep mtsToCreate total size in sync with stats.TotalPointsWaitingToBeCreated
        mtsToCreate     chan []*datum.Datum
        maxCreationSize *distconf.Int
        stats           cacheStats
        ctx             context.Context
        //        MtsCreationDebug  func(dat *datum.Datum) bool

        timekeeper timekeeper.TimeKeeper

        client *sfxclient.Reporter
}

// TsidLookupConfig stores configuration information for the InjectTsidOnDatum object
type TsidLookupConfig struct {
        MaxWaitSize     *distconf.Int
        MaxBatchSize    *distconf.Int
        TimeoutDuration *distconf.Duration
        ParallelBatches *distconf.Int
}

// Load the cache config from distconf
func (d *TsidLookupConfig) Load(conf *distconf.Config) <span class="cov8" title="1">{
        d.MaxWaitSize = conf.Int("sf.sbingest.tsidcache.max_datapoint_wait_size", 2000000)
        d.MaxBatchSize = conf.Int("sf.sbingest.tsidcache.datapoint_batch_size", 500)
        d.ParallelBatches = conf.Int("sf.sbingest.tsidcache.parallel_batches", 10)
        d.TimeoutDuration = conf.Duration("sf.sbingest.tsidcache.timeout_duration", time.Second*5)
}</span>

// New creates a new service to inject tsids, starting background goroutines
func New(ctx context.Context, datumOutput datumsink.Sink, timeSeriesCreatorFactory func() TimeSeriesCreator, diskCache *tsiddiskcache.DiskCache, client *sfxclient.Reporter, config *TsidLookupConfig) *InjectTsidOnDatum <span class="cov8" title="1">{
        ret := &amp;InjectTsidOnDatum{
                diskCache:       diskCache,
                datumOutput:     datumOutput,
                closeChan:       make(chan struct{}),
                mtsToCreate:     make(chan []*datum.Datum, 200000),
                maxWaitSize:     config.MaxWaitSize,
                maxCreationSize: config.MaxBatchSize,
                client:          client,
                ctx:             ctx,
                logLimiter:      eventcounter.New(time.Now(), time.Second*2),
                timeoutTracker: orgTimeoutTracker{
                        orgsInTimeout:   make(map[int64]time.Time, 8),
                        timeoutDuration: config.TimeoutDuration,
                },
                timekeeper: timekeeper.RealTime{},
        }
        ret.setupMetricReporting()
        // Note: Important to add them before we move on in case we Close() before we start the below
        //       goroutines
        endMtsCreationLoopSignal := make(chan struct{}, 1024)

        updateRunningGoroutines := func(str *distconf.Int, currentlyNumRunningCreationLoops int64) </span><span class="cov8" title="1">{
                select </span>{
                <span class="cov8" title="1">case &lt;-ret.closeChan:
                        return</span>
                <span class="cov8" title="1">default:</span>
                }
                <span class="cov8" title="1">newNumRunningCreationLoops := str.Get()
                for newNumRunningCreationLoops &lt; currentlyNumRunningCreationLoops </span><span class="cov8" title="1">{
                        endMtsCreationLoopSignal &lt;- struct{}{}
                        currentlyNumRunningCreationLoops--
                }</span>
                <span class="cov8" title="1">for newNumRunningCreationLoops &gt; currentlyNumRunningCreationLoops </span><span class="cov8" title="1">{
                        ret.spawnedGoroutinesClosed.Add(1)
                        atomic.AddInt64(&amp;ret.stats.NumCurrentlyRunningMtsCreationLoops, 1)
                        go ret.mtsCreationLoop(timeSeriesCreatorFactory(), endMtsCreationLoopSignal)
                        currentlyNumRunningCreationLoops++
                }</span>
                <span class="cov8" title="1">atomic.AddInt64(&amp;ret.stats.NumThreadUpdates, 1)</span>
        }

        <span class="cov8" title="1">updateRunningGoroutines(config.ParallelBatches, 0)

        config.ParallelBatches.Watch(updateRunningGoroutines)

        return ret</span>
}

// HitsMisses is used by checkpointing to load and store cache hits and misses
type HitsMisses struct {
        hits   int64
        misses int64
}

func (h *HitsMisses) sub(m *HitsMisses) *HitsMisses <span class="cov8" title="1">{
        return &amp;HitsMisses{
                hits:   h.hits - m.hits,
                misses: h.misses - m.misses,
        }
}</span>

// LoadStats stores into storeInto the current state of disk cache hits and misses
func (i *InjectTsidOnDatum) LoadStats(storeInto *HitsMisses) *HitsMisses <span class="cov8" title="1">{
        storeInto.hits = atomic.LoadInt64(&amp;i.stats.DiskCacheHits)
        storeInto.misses = atomic.LoadInt64(&amp;i.stats.DiskCacheMiss)
        return storeInto
}</span>

// CheckpointDisk will attempt to checkpoint the on disk cache if the hit ratio is high enough
func (i *InjectTsidOnDatum) CheckpointDisk(lastStats *HitsMisses) (currentStats *HitsMisses) <span class="cov8" title="1">{
        currentStats = i.LoadStats(&amp;HitsMisses{})
        delta := currentStats.sub(lastStats)
        deltaTotal := delta.hits + delta.misses
        if deltaTotal == 0 </span><span class="cov8" title="1">{
                return
        }</span>
        <span class="cov8" title="1">hitPercent := float64(delta.hits) / float64(deltaTotal)
        if hitPercent &gt; .9 </span><span class="cov8" title="1">{
                log.WithField("per", hitPercent).Info("Checkpointing database")
                err := i.diskCache.MarkValidAtTimestamp(i.timekeeper.Now())
                if err != nil </span><span class="cov8" title="1">{
                        log.WithField("err", err).Warn("Unable to checkpoint timestamp")
                }</span>
        }
        <span class="cov8" title="1">return</span>
}

func (i *InjectTsidOnDatum) setupMetricReporting() <span class="cov8" title="1">{
        dims := map[string]string{
                "instance": "tsidcache",
        }

        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.PointDroppedContextClosed)).Dimensions(dims).AppendDimensions(map[string]string{"reason": "context_closed"})
        i.client.Cumulative("DiskCacheHits", sfxclient.Int(&amp;i.stats.DiskCacheHits)).Dimensions(dims)
        i.client.Cumulative("DiskCacheMiss", sfxclient.Int(&amp;i.stats.DiskCacheMiss)).Dimensions(dims)

        i.client.Cumulative("DiskCacheErrRead", sfxclient.Int(&amp;i.stats.DiskCacheErrRead)).Dimensions(dims)
        i.client.Cumulative("DiskCacheErrWrite", sfxclient.Int(&amp;i.stats.DiskCacheErrWrite)).Dimensions(dims)

        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.PointsDroppedPerStatus[datamodel.CreateManyResultStatus_EXCEPTION])).Dimensions(dims).AppendDimensions(map[string]string{"reason": "exception_on_item_tsid_creation"})
        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.PointsDroppedPerStatus[datamodel.CreateManyResultStatus_THROTTLED])).Dimensions(dims).AppendDimensions(map[string]string{"reason": "tsid_creation_throttle", "from": "throttle"})
        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.PointsDroppedPerStatus[datamodel.CreateManyResultStatus_LIMITED])).Dimensions(dims).AppendDimensions(map[string]string{"reason": "tsid_creation_limited", "from": "throttle"})
        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.PointsDroppedUnknownStatus)).Dimensions(dims).AppendDimensions(map[string]string{"reason": "unknown_tsid_item_result"})

        i.client.Cumulative("create_many_result_count", sfxclient.Int(&amp;i.stats.TotalItemResults[datamodel.CreateManyResultStatus_EXCEPTION])).Dimensions(dims).AppendDimensions(map[string]string{"type": "exception"})
        i.client.Cumulative("create_many_result_count", sfxclient.Int(&amp;i.stats.TotalItemResults[datamodel.CreateManyResultStatus_THROTTLED])).Dimensions(dims).AppendDimensions(map[string]string{"type": "throttled"})
        i.client.Cumulative("create_many_result_count", sfxclient.Int(&amp;i.stats.TotalItemResults[datamodel.CreateManyResultStatus_LIMITED])).Dimensions(dims).AppendDimensions(map[string]string{"type": "limited"})
        i.client.Cumulative("create_many_result_count", sfxclient.Int(&amp;i.stats.TotalItemResults[datamodel.CreateManyResultStatus_CREATED])).Dimensions(dims).AppendDimensions(map[string]string{"type": "created"})
        i.client.Cumulative("create_many_result_count", sfxclient.Int(&amp;i.stats.TotalItemResults[datamodel.CreateManyResultStatus_FETCHED])).Dimensions(dims).AppendDimensions(map[string]string{"type": "fetched"})
        i.client.Cumulative("create_many_result_count", sfxclient.Int(&amp;i.stats.TotalItemResultsUnknown)).Dimensions(dims).AppendDimensions(map[string]string{"type": "unknown"})

        i.client.Cumulative("TotalMtsCreationCalls", sfxclient.Int(&amp;i.stats.TotalMtsCreationCalls)).Dimensions(dims)
        i.client.Cumulative("TotalMtsCreated", sfxclient.Int(&amp;i.stats.TotalMtsCreated)).Dimensions(dims)

        i.client.Cumulative("TotalMtsCreationErrors", sfxclient.Int(&amp;i.stats.TotalMtsCreationErrors)).Dimensions(dims)
        // This is when the entire call returns an error, not an exception
        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.TotalMtsDroppedByBadCreation)).Dimensions(dims).AppendDimensions(map[string]string{"reason": "tsid_creation_thrift_error"})
        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.TotalDroppedInTimeout)).Dimensions(dims).AppendDimensions(map[string]string{"reason": "org_mtscreate_in_timeout"})

        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.DropBacklogFull)).Dimensions(dims).AppendDimensions(map[string]string{"reason": "tsid_backlog_full"})

        i.client.Cumulative("TotalMtsCreationExceptions", sfxclient.Int(&amp;i.stats.TotalMtsCreationExceptions)).Dimensions(dims)
        // This is when the entire call excepts, not just a single item
        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.TotalMtsDroppedByExceptionCreation)).Dimensions(dims).AppendDimensions(map[string]string{"reason": "tsid_creation_metabase_exception"})

        i.client.Cumulative("dropped_points", sfxclient.Int(&amp;i.stats.DropNoTsAfterCreate)).Dimensions(dims).AppendDimensions(map[string]string{"reason": "no_tsid_after_creation"})

        i.client.Gauge("BacklogSize", sfxclient.IntVal(func() int64 </span><span class="cov8" title="1">{ return int64(len(i.mtsToCreate)) }</span>)).Dimensions(dims)
        <span class="cov8" title="1">i.client.Gauge("PointsBacklogSize", sfxclient.Int(&amp;i.stats.TotalPointsWaitingToBeCreated)).Dimensions(dims)</span>
}

// Close this service and end any waiting goroutines
func (i *InjectTsidOnDatum) Close() <span class="cov8" title="1">{
        close(i.closeChan)
        // We want to wait for spawned goroutines to finish because the disk may be closed or deleted
        // next and we want to be done with the disk before we move on
        i.spawnedGoroutinesClosed.Wait()
}</span>

var errInternalBufferFull = errors.New("internal buffer full: try again later")

// AddDatum forwards points that either have a TSID or are in the disk cache already, and will
// buffer points whos TSID needs to be fetched.  Returns an error if the buffer is full.
func (i *InjectTsidOnDatum) AddDatum(ctx context.Context, points []*datum.Datum) error <span class="cov8" title="1">{
        points, _ = i.forwardPointsAlreadyWithTsID(ctx, points)
        points, _ = i.forwardPointsInDiskCache(ctx, points)
        if len(points) == 0 </span><span class="cov8" title="1">{
                return nil
        }</span>
        <span class="cov8" title="1">atomic.AddInt64(&amp;i.stats.DiskCacheMiss, int64(len(points)))

        if atomic.LoadInt64(&amp;i.stats.TotalPointsWaitingToBeCreated) &gt; i.maxWaitSize.Get() </span><span class="cov8" title="1">{
                atomic.AddInt64(&amp;i.stats.DropBacklogFull, int64(len(points)))
                return errInternalBufferFull
        }</span>

        <span class="cov8" title="1">select </span>{
        <span class="cov8" title="1">case i.mtsToCreate &lt;- points:
                atomic.AddInt64(&amp;i.stats.TotalPointsWaitingToBeCreated, int64(len(points)))
                logherd.Debug(log, "", "", "Sent to create")
                return nil</span>
        <span class="cov8" title="1">case &lt;-ctx.Done():
                atomic.AddInt64(&amp;i.stats.PointDroppedContextClosed, int64(len(points)))
                log.Warn("Context closed.  Dropping points")
                return ctx.Err()</span>
        }
}

func (i *InjectTsidOnDatum) forwardPointsAlreadyWithTsID(ctx context.Context, points []*datum.Datum) ([]*datum.Datum, error) <span class="cov8" title="1">{
        pointsWithTsID := make([]*datum.Datum, 0, len(points))
        pointsWithoutTsID := make([]*datum.Datum, 0, len(points))
        for _, dat := range points </span><span class="cov8" title="1">{
                if dat.TsID != 0 </span><span class="cov8" title="1">{
                        logherd.Debug(log, "dat", dat, "Sending datum that already has tsid")
                        pointsWithTsID = append(pointsWithTsID, dat)
                        continue</span>
                }
                <span class="cov8" title="1">pointsWithoutTsID = append(pointsWithoutTsID, dat)</span>
        }
        <span class="cov8" title="1">var err error
        if len(pointsWithTsID) != 0 </span><span class="cov8" title="1">{
                err = i.datumOutput.AddDatum(ctx, pointsWithTsID)
        }</span>
        <span class="cov8" title="1">return pointsWithoutTsID, err</span>
}

func (i *InjectTsidOnDatum) forwardPointsInDiskCache(ctx context.Context, points []*datum.Datum) ([]*datum.Datum, error) <span class="cov8" title="1">{
        pointsInDiskCache := make([]*datum.Datum, 0, len(points))
        pointsNotInDiskCache := make([]*datum.Datum, 0, len(points))
        buf := new(bytes.Buffer)
        lookupEncodables := make([]tsiddiskcache.Encodeable, len(points))
        for i := range points </span><span class="cov8" title="1">{
                buf.Reset()
                lookupEncodables[i] = (&amp;tsiddiskcache.DiskKey{}).FromDatum(points[i])
        }</span>
        <span class="cov8" title="1">ids, err := i.diskCache.Tsid(buf, lookupEncodables)
        if err != nil </span><span class="cov8" title="1">{
                atomic.AddInt64(&amp;i.stats.DiskCacheErrRead, 1)
                // Create will fail the write, but still forward the point
                log.WithField("err", err).Warn("Error getting tsid.  Database may be corrupt")
                return points, err
        }</span>
        <span class="cov8" title="1">for i, id := range ids </span><span class="cov8" title="1">{
                dat := points[i]
                if id &gt; 0 </span><span class="cov8" title="1">{
                        dat.TsID = id
                        logherd.Debug(log, "dat", dat, "ID exists, forwarding")
                        pointsInDiskCache = append(pointsInDiskCache, dat)
                        continue</span>
                }
                <span class="cov8" title="1">pointsNotInDiskCache = append(pointsNotInDiskCache, dat)</span>
        }
        <span class="cov8" title="1">if len(pointsInDiskCache) != 0 </span><span class="cov8" title="1">{
                atomic.AddInt64(&amp;i.stats.DiskCacheHits, int64(len(pointsInDiskCache)))
                err = i.datumOutput.AddDatum(ctx, pointsInDiskCache)
        }</span>
        <span class="cov8" title="1">return pointsNotInDiskCache, err</span>
}

func (i *InjectTsidOnDatum) mtsCreationLoop(timeSeriesCreator TimeSeriesCreator, endMtsCreationLoopSignal &lt;-chan struct{}) <span class="cov8" title="1">{
        defer i.spawnedGoroutinesClosed.Done()
        defer atomic.AddInt64(&amp;i.stats.NumCurrentlyRunningMtsCreationLoops, -1)
        log.Info("Starting mtsCreationLoop")
        defer log.Info("Done with mtsCreationLoop")
        for </span><span class="cov8" title="1">{
                select </span>{
                <span class="cov8" title="1">case &lt;-i.closeChan:
                        return</span>
                <span class="cov8" title="1">case &lt;-endMtsCreationLoopSignal:
                        return</span>
                <span class="cov8" title="1">case dat := &lt;-i.mtsToCreate:
                        atomic.AddInt64(&amp;i.stats.TotalPointsWaitingToBeCreated, -int64(len(dat)))
                        i.drainAndCreateMts(timeSeriesCreator, dat)</span>
                }
        }
}

func (i *InjectTsidOnDatum) drainMtsToCreate(dat []*datum.Datum) []*datum.Datum <span class="cov8" title="1">{
        dat = i.fastFilterDatum(dat)
outer:
        for int64(len(dat)) &lt; i.maxCreationSize.Get() </span><span class="cov8" title="1">{
                select </span>{
                <span class="cov8" title="1">case d := &lt;-i.mtsToCreate:
                        atomic.AddInt64(&amp;i.stats.TotalPointsWaitingToBeCreated, -int64(len(d)))
                        dat = i.fastFilterDatum(dat)
                        dat = append(dat, d...)</span>
                <span class="cov8" title="1">default:
                        break outer</span>
                }
        }
        <span class="cov8" title="1">return dat</span>
}

func (i *InjectTsidOnDatum) fastFilterDatum(dat []*datum.Datum) []*datum.Datum <span class="cov8" title="1">{
        dat, _ = i.forwardPointsInDiskCache(i.ctx, dat)
        dat = i.filterOrgsInTimeout(dat)
        return dat
}</span>

func (i *InjectTsidOnDatum) filterOrgsInTimeout(dat []*datum.Datum) []*datum.Datum <span class="cov8" title="1">{
        now := i.timekeeper.Now()
        ret := make([]*datum.Datum, 0, len(dat))
        totalDroppedInTimeout := int64(0)
        for _, datum := range dat </span><span class="cov8" title="1">{
                if i.timeoutTracker.IsOrgInTimeout(now, datum.Org()) </span><span class="cov8" title="1">{
                        totalDroppedInTimeout++
                        continue</span>
                }
                <span class="cov8" title="1">ret = append(ret, datum)</span>
        }
        <span class="cov8" title="1">atomic.AddInt64(&amp;i.stats.TotalDroppedInTimeout, totalDroppedInTimeout)
        return ret</span>
}

func (i *InjectTsidOnDatum) drainAndCreateMts(timeSeriesCreator TimeSeriesCreator, dat []*datum.Datum) <span class="cov8" title="1">{
        dat = i.drainMtsToCreate(dat)
        dat = i.fastFilterDatum(dat)
        if len(dat) == 0 </span><span class="cov8" title="1">{
                return
        }</span>
        <span class="cov8" title="1">logherd.Debug(log, "dat", len(dat), "Creating a MTS")
        metricTimeSeries := make(map[*datamodel.MetricTimeSeriesDetails]bool)
        mtsDoublecreateCheck := make(map[string]struct{})
        buf := new(bytes.Buffer)
        for _, da := range dat </span><span class="cov8" title="1">{
                mtsd := datamodel.NewMetricTimeSeriesDetails()
                mtsd.OrganizationId = id.NewID()
                storeMetricTimeSeriesDetails(mtsd, da)
                buf.Reset()
                (&amp;tsiddiskcache.DiskKey{}).FromMTSD(mtsd).EncodeCycle(buf)
                s := buf.String()
                _, exists := mtsDoublecreateCheck[s]
                if !exists </span><span class="cov8" title="1">{
                        mtsDoublecreateCheck[s] = struct{}{}
                        metricTimeSeries[mtsd] = true
                }</span>
        }
        <span class="cov8" title="1">if i.logLimiter.Event(i.timekeeper.Now()) &lt;= 1 </span><span class="cov8" title="1">{
                log.WithField("dat", dat[0].String()).Info("MTS not in disk")
        }</span>
        <span class="cov8" title="1">logherd.Debug(log, "len(metricTimeSeries)", len(metricTimeSeries), "Creating TS")
        atomic.AddInt64(&amp;i.stats.TotalMtsCreationCalls, 1)
        atomic.AddInt64(&amp;i.stats.TotalMtsCreated, int64(len(metricTimeSeries)))
        res, err := repeatedTrying(timeSeriesCreator, metricTimeSeries, map[*datamodel.EventTimeSeriesDetails]bool{}, false)
        logherd.Debug2(log, "err", err, "res", res, "CreateManyTimeSeries res")
        if err != nil </span><span class="cov8" title="1">{
                ex, ok := err.(*metabase.MetabaseException)
                if ok </span><span class="cov8" title="1">{
                        atomic.AddInt64(&amp;i.stats.TotalMtsCreationExceptions, 1)
                        atomic.AddInt64(&amp;i.stats.TotalMtsDroppedByExceptionCreation, int64(len(metricTimeSeries)))
                        log.WithField("ex", ex).Warn("Unable to create metric time series with exception.  Dropping points")
                }</span><span class="cov8" title="1"> else {
                        atomic.AddInt64(&amp;i.stats.TotalMtsCreationErrors, 1)
                        atomic.AddInt64(&amp;i.stats.TotalMtsDroppedByBadCreation, int64(len(metricTimeSeries)))
                        log.WithField("err", err).Warn("Unable to create metric time series.  Dropping points")
                }</span>
                <span class="cov8" title="1">return</span>
        }
        <span class="cov8" title="1">buf.Reset()
        if err = i.diskCache.WriteResult(buf, tsiddiskcache.FromThrift(res)); err != nil </span><span class="cov8" title="1">{
                atomic.AddInt64(&amp;i.stats.DiskCacheErrWrite, 1)
                log.WithField("err", err).Warn("Unable to write result to on disk cache")
        }</span>
        <span class="cov8" title="1">idRes := i.inMemoryResultsFromRes(buf, res.MetricTimeSeriesResultObjs)
        i.processResultAfterThrift(buf, dat, idRes, timeSeriesCreator)
        logherd.Debug(log, "", "", "Sending creation result to chan")</span>
}

type idRes interface {
        ID() (*int64, datamodel.CreateManyResultStatus)
}

type idResFromThrift struct {
        *datamodel.MTSCreateResult_
}

func (i idResFromThrift) ID() (*int64, datamodel.CreateManyResultStatus) <span class="cov8" title="1">{
        // I return nil to enforce the ID is only used if the status is valid
        if i.Status == datamodel.CreateManyResultStatus_CREATED || i.Status == datamodel.CreateManyResultStatus_FETCHED </span><span class="cov8" title="1">{
                return &amp;i.Mts.Id.Id, i.Status
        }</span>
        <span class="cov8" title="1">return nil, i.Status</span>
}

func (i *InjectTsidOnDatum) inMemoryResultsFromRes(buf *bytes.Buffer, mapObjs map[*datamodel.MetricTimeSeriesDetails]*datamodel.MTSCreateResult_) map[string]idRes <span class="cov8" title="1">{
        idRes := make(map[string]idRes, len(mapObjs))
        for k, v := range mapObjs </span><span class="cov8" title="1">{
                buf.Reset()
                (&amp;tsiddiskcache.DiskKey{}).FromMTSD(k).EncodeCycle(buf)
                idx := int(v.Status)
                if idx &gt;= 0 &amp;&amp; idx &lt; len(i.stats.TotalItemResults) </span><span class="cov8" title="1">{
                        atomic.AddInt64(&amp;i.stats.TotalItemResults[idx], 1)
                }</span><span class="cov8" title="1"> else {
                        atomic.AddInt64(&amp;i.stats.TotalItemResultsUnknown, 1)
                }</span>
                <span class="cov8" title="1">idRes[string(buf.String())] = idResFromThrift{v}</span>
        }
        <span class="cov8" title="1">return idRes</span>
}

func (i *InjectTsidOnDatum) processDatumAfterThriftCall(foundStrings map[string]struct{}, buf *bytes.Buffer, idr idRes, datu *datum.Datum, now time.Time) <span class="cov8" title="1">{
        foundStrings[buf.String()] = struct{}{}
        id, status := idr.ID()
        if id != nil </span><span class="cov8" title="1">{
                datu.TsID = *id
        }</span><span class="cov8" title="1"> else {
                idx := int(status)
                if idx &gt;= 0 &amp;&amp; idx &lt; len(i.stats.PointsDroppedPerStatus) </span><span class="cov8" title="1">{
                        if status == datamodel.CreateManyResultStatus_LIMITED || status == datamodel.CreateManyResultStatus_THROTTLED </span><span class="cov8" title="1">{
                                i.timeoutTracker.PutOrgInTimeout(now, datu.Org())
                        }</span>
                        <span class="cov8" title="1">atomic.AddInt64(&amp;i.stats.PointsDroppedPerStatus[idx], 1)</span>
                }<span class="cov8" title="1"> else {
                        atomic.AddInt64(&amp;i.stats.PointsDroppedUnknownStatus, 1)
                }</span>
        }
}

func (i *InjectTsidOnDatum) processResultAfterThrift(buf *bytes.Buffer, datumToCreateCache []*datum.Datum, idRes map[string]idRes, timeSeriesCreator TimeSeriesCreator) <span class="cov8" title="1">{
        totalUnfound := int64(0)
        now := i.timekeeper.Now()
        // TODO: Remove me when done debugging
        foundStrings := make(map[string]struct{}, len(idRes))
        for _, datu := range datumToCreateCache </span><span class="cov8" title="1">{
                buf.Reset()
                (&amp;tsiddiskcache.DiskKey{}).FromDatum(datu).EncodeCycle(buf)
                idr, exists := idRes[buf.String()]
                if exists </span><span class="cov8" title="1">{
                        i.processDatumAfterThriftCall(foundStrings, buf, idr, datu, now)
                }</span><span class="cov8" title="1"> else {
                        totalUnfound++
                }</span>
        }
        <span class="cov8" title="1">if totalUnfound != 0 </span><span class="cov8" title="1">{
                atomic.AddInt64(&amp;i.stats.DropNoTsAfterCreate, totalUnfound)
        outer:
                for encodedMtsd := range idRes </span><span class="cov8" title="1">{
                        if _, exists := foundStrings[encodedMtsd]; !exists </span><span class="cov8" title="1">{
                                log.WithField("totalUnfound", totalUnfound).WithField("Encoded", encodedMtsd).Info("Debug this")
                                break outer</span>
                        }
                }
        }
        <span class="cov8" title="1">_, err := i.forwardPointsAlreadyWithTsID(i.ctx, datumToCreateCache)
        if err != nil </span><span class="cov8" title="1">{
                err2 := timeSeriesCreator.CheckHealth()
                log.WithField("err", err).WithField("err2", err2).Info("Has drops.  Doing health check just in case.")
        }</span>
}

func repeatedTrying(arg0 TimeSeriesCreator, arg1 map[*datamodel.MetricTimeSeriesDetails]bool, arg2 map[*datamodel.EventTimeSeriesDetails]bool, arg3 bool) (r *datamodel.CreateManyResult_, err error) <span class="cov8" title="1">{
        for i := 0; i &lt; 3; i++ </span><span class="cov8" title="1">{
                // Will do nothing if already connected
                e1 := arg0.(sfxthrift.ReconnectingTransport).ReconnectTransport()
                if e1 != nil </span><span class="cov8" title="1">{
                        return nil, e1
                }</span>
                <span class="cov8" title="1">logherd.Debug(log, "i", i, "Trying write")
                r, err = arg0.CreateManyTimeSeries(arg1, arg2, arg3)
                if err == nil </span><span class="cov8" title="1">{
                        return
                }</span>
                <span class="cov8" title="1">_, ok := err.(thrift.TTransportException)
                if !ok </span><span class="cov8" title="1">{
                        log.WithField("err", err).Warn("Unable to property execute thrift request")
                        return
                }</span>
                // Will reconnect and try again
        }
        <span class="cov8" title="1">return</span>
}

func storeMetricTimeSeriesDetails(storeInto *datamodel.MetricTimeSeriesDetails, d *datum.Datum) <span class="cov8" title="1">{
        storeInto.OBSOLETEProperties = nil
        storeInto.Properties = nil
        storeInto.Source = ""
        storeInto.OrganizationId.Id = d.OrgID
        storeInto.Metric = d.Metric
        storeInto.MetricType = toThriftType(d.MetricType)
        storeInto.Dimensions = d.Dimensions
}</span>

func getPointer(m datamodel.MetricType) *datamodel.MetricType <span class="cov8" title="1">{
        return &amp;m
}</span>

func toThriftType(mt datapoint.MetricType) *datamodel.MetricType <span class="cov8" title="1">{
        if mt == datapoint.Gauge </span><span class="cov8" title="1">{
                return getPointer(datamodel.MetricType_GAUGE)
        }</span>
        <span class="cov8" title="1">if mt == datapoint.Counter </span><span class="cov8" title="1">{
                return getPointer(datamodel.MetricType_CUMULATIVE_COUNTER)
        }</span>
        <span class="cov8" title="1">if mt == datapoint.Count </span><span class="cov8" title="1">{
                return getPointer(datamodel.MetricType_COUNTER)
        }</span>
        <span class="cov8" title="1">if mt == datapoint.Enum </span><span class="cov8" title="1">{
                return getPointer(datamodel.MetricType_ENUM)
        }</span>
        <span class="cov8" title="1">log.WithField("mt", mt).Warn("Unknown metric type. Using gauge")
        return getPointer(datamodel.MetricType_GAUGE)</span>
}
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible = document.getElementById('file0');
		files.addEventListener('change', onChange, false);
		function onChange() {
			visible.style.display = 'none';
			visible = document.getElementById(files.value);
			visible.style.display = 'block';
			window.scrollTo(0, 0);
		}
	})();
	</script>
</html>
